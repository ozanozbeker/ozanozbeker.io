---
format:
  html: 
    toc: true
    toc-title: "Sections"
    other-links:
      - text: "Cover Letter PDF"
        href: resources/documents/ozan-ozbeker-cover-letter.pdf
        icon: file-earmark-text
      - text: "Resume PDF"
        href: resources/documents/ozan-ozbeker-resume.pdf
        icon: file-earmark-text
      - text: "GitHub"
        href: https://github.com/ozanozbeker/
        icon: github
      - text: "LinkedIn"
        href: https://www.linkedin.com/in/ozanozbeker/
        icon: linkedin
      - text: "Email"
        href: mailto:ozanozbeker99@outlook.com
        icon: envelope
page-layout: full
comments: false
---

# Summary {#sec-summary .title}

## Data Analytics & Consulting

As a data scientist, I specialize in driving impactful projects from inception to completion for clients in the replacement parts business. My work supports critical decision-making processes through comprehensive requirements gathering, seamless data integration from various sources, and the creation of insightful reports and dashboards. These tools not only inform decisions but also help shape strategic directions in stocking and pricing of products.

In my previous roles, I honed my skills in managing Excess and Obsolete (E&O) inventory. I developed automated workflows and robust SQL queries to analyze and handle inventory efficiently, aligning the ETL process with company policies. My responsibilities included significant cross-departmental collaboration, working with finance, procurement, logistics, sustaining engineering, and product management to standardize workflows and form teams for monthly E&O inventory reviews.

Presenting to clients is one of my strengths. I manage comprehensive data projects, from gathering requirements to delivering insightful results. This involves conducting thorough client interviews and presenting clear, actionable insights. Internally, I've partnered with various business functions to meet data pipeline and dashboard needs, regularly presenting findings to Director and VP-level stakeholders.

## Core Competencies

::: panel-tabset
## R Programming

I bring a high level of proficiency in R to my professional work, with a strong emphasis on the Tidyverse, Tidymodels, and related packages. As a "full-stack" data scientist, I manage the entire data science process framework, covering the Import, Tidy, Transform, Visualize, Model, and Communicate stages.

### Key Areas of Expertise

#### Import (& Data Management)
- **Data Sources:** Extensive experience importing from databases, spreadsheets, web scraping, and APIs.
- **Database Connections:** Accessing client data through SQL Server, PostgreSQL, and SAP HANA.
- **Web Scraping and Automation:** Creating and automating scrapes and API calls for data collection from services like Amazon and Walmart. Using Selenium to automate browsers and bypass bot detection.
- **Recent Tools:** Working with DuckDB for local data analysis and storage, though it is less frequently needed due to ample memory on work servers.
- **Data Management:** Managing PostgreSQL databases for clients, including data modeling and management.
- **Key Tools:** [{httr2}](https://httr2.r-lib.org/), [{rvest}](https://rvest.tidyverse.org/index.html), [{RSelenium}](https://docs.ropensci.org/RSelenium/index.html), [{DBI}](https://dbi.r-dbi.org/), [DuckDB](https://duckdb.org/), [Oxylabs](https://oxylabs.io/), [Helium 10](https://www.helium10.com/)

#### Tidy & Transform
- **Data Wrangling:** Extensive experience in focusing on grouping, filtering, and summarizing data.
- **Data Tidying:** Transforming data by pivoting and converting data types (e.g., strings to dates, floats).
- **Regex Expertise:** Skilled in using regular expressions for data cleaning and manipulation.
- **Key Tools:** [{tidyverse}](https://www.tidyverse.org/), [{janitor}](https://sfirke.github.io/janitor/index.html)

#### Visualize:
- **Exploratory Data Analysis:** Frequently use {ggplot2} and its extensions for data visualization.
- **Client Preferences:** While initial exploration and cleaning are done in R, clients prefer Tableau or other BI tools for final dashboards.
- **Key Tools:** [{ggplot2}](https://ggplot2.tidyverse.org/), [{gt}](https://gt.rstudio.com/)

#### Model:
- **Forecasting and Classification:** Professional experience in forecasting with linear regression models and classification with support vector machines and recommendation engines for pricing.
- **Key Tools:** [Base R Stats](https://rdrr.io/r/stats/stats-package.html), [{tidymodels}](https://www.tidymodels.org/)

#### Communicate:
- **Automated Reports:** Building automated reports for clients to support financial decision-making, including dashboards and executive reports.
- **Customization:** Using HTML and CSS to customize and enhance the visual appeal of reports. This website was built with Quarto and edited with custom CSS.
- **Key Tools:** [Quarto](https://quarto.org/), [{htmlwidgets}](https://www.htmlwidgets.org/), [{htmltools}](https://rstudio.github.io/htmltools/index.html)

#### Programming:
- **Functional Programming:** Using the {purrr} package for mapping functions and other utilities within the Tidyverse framework.
- **Version Control:** Incorporating Git and GitHub into workflows and managing project structure.
- **Key Tools:** Task Scheduler, Git & GitHub, [{fs}](https://fs.r-lib.org/)

## Tableau, Tableau Prep, & Power BI

### Tableau Experience

I have developed a strong competency in Tableau through both formal education and practical application in professional settings. Here is a detailed summary of my Tableau experience:

-   **Learning and Initial Skills Development:**
    -   Gained foundational knowledge of Tableau through DataCamp's introductory course, which covered the most common functions and features.
    -   Acquired skills to create various charts and dashboards tailored to managerial needs.
-   **Practical Applications:**
    -   **Dashboard Creation:**
        -   Created dashboards linked to the company's ERP system, primarily displaying aggregated data for daily, weekly, and monthly performance metrics.
        -   Developed dashboards for clients that connect to their specific data sources, published on their Tableau Servers. These dashboards cover:
            -   Parts distribution across distributors.
            -   Performance metrics for Amazon marketplace.
            -   Internal supply chain performance monitoring.
        -   Utilized a mix of database connections, custom ETL processes, and multifaceted dashboards combining tables and charts for comprehensive stakeholder reporting.
    -   **Tableau Prep for ETL:**
        -   Used Tableau Prep for ETL tasks to prepare data for analysis and visualization in Tableau. Despite finding Tableau Prep to be slow, this experience motivated me to learn SQL for more efficient data processing.
-   **Advanced Tableau Functions:**
    -   Maintained multiple dashboards with regular updates and enhancements based on client feedback and evolving data needs.
    -   Ensured data integrity and accuracy through rigorous testing and validation of ETL processes and dashboard outputs.

### Power BI Experience

My experience with Power BI stems from a company-wide transition from Tableau to Power BI, where I acquired the basics and applied them in a professional context. Here is a detailed summary of my Power BI experience:

-   **Learning and Initial Skills Development:**
    -   Gained foundational knowledge of Power BI during a transition from Tableau to Power BI as part of a Microsoft 365 (M365) integration at my previous company.
    -   Learned the basics of creating visualizations, reports, and dashboards within Power BI.
-   **Practical Applications:**
    -   Applied Power BI skills to create and maintain reports and dashboards, similar to those previously developed in Tableau.
    -   Utilized Power BI's features to connect to various data sources, perform data transformations, and build interactive visualizations.

## SQL & Python

### SQL Professional Experience

I have significant professional experience in creating complex SQL queries to generate custom reports for internal customers. This involves:

-   **Complex Queries:**
    -   **Common Table Expressions (CTEs):** Utilizing CTEs for better readability and modular query design.
    -   **Subqueries:** Embedding subqueries within main queries for dynamic data retrieval.
    -   **Nested Case-When Statements:** Implementing conditional logic within queries to handle complex data manipulation.
    -   **Aggregate Calculations:** Performing aggregations such as SUM, AVG, COUNT, etc., to summarize data.
    -   **Filtering:** Applying WHERE clauses and HAVING conditions to filter data as per requirements.
    -   **Date/Time Functions:** Manipulating and formatting date and time data.
    -   **String Functions:** Using functions like CONCAT, SUBSTRING, and LENGTH for string manipulation.

The most challenging aspects of reporting often involve not the SQL code itself but the process of identifying and validating the required data sources. Ensuring data accuracy and consistency is paramount in delivering reliable reports.

#### Continued Education and Advanced Skills

In my continuous pursuit of knowledge and skill enhancement, I have learned and practiced several advanced SQL techniques, including:

-   **Window Functions:** Applying functions like ROW_NUMBER, RANK, and LEAD/LAG to perform calculations across specific partitions of data.
-   **Transactions and Error Handling:** Implementing transaction control (BEGIN, COMMIT, ROLLBACK) and error handling mechanisms to ensure data integrity.
-   **Arrays:** Using arrays for more complex data structures and operations.
-   **Pivoting Tables:** Creating pivot tables using the CROSSTAB function to transform data from rows to columns.
-   **Creating Totals and Grand Totals:**
    -   **ROLLUP:** Generating subtotals and grand totals in query results.
    -   **CUBE:** Extending ROLLUP to include cross-tabulated totals.

#### SQL Varieties and Environments

-   **PostgreSQL:**
    -   Most of my SQL experience is in PostgreSQL, where I have developed and executed a wide range of queries and scripts.
-   **Other SQL Environments:**
    -   **MS Access:** Occasionally used the SQL editor in MS Access for database management tasks.
    -   **SQL Server:** Completed a formal class in school focused on SQL Server, gaining foundational knowledge and skills in this environment.

### Python Teaching Experience

-   **Undergraduate Teaching Assistant:**
    -   While I haven't used Python in a professional setting, my proficiency in Python was recognized when I served as an undergraduate teaching assistant. In this role, I:
        -   Assisted students in understanding and applying Python for data analysis and visualization.
        -   Helped design and grade assignments, projects, and exams.
        -   Conducted review sessions and provided one-on-one tutoring to students.
    -   Although I haven't used Python extensively in a professional setting recently, my foundational skills remain strong. I have a solid understanding of the core libraries and techniques needed for data manipulation, analysis, and visualization.

#### Key Python Skills

-   **Pandas:** Data manipulation, cleaning, aggregation, and merging.
-   **NumPy:** Numerical computations, array manipulation, and mathematical functions.
-   **Matplotlib:** Creating static visualizations, customizing plots, and adjusting aesthetics.
-   **Seaborn:** Advanced statistical visualizations, integrating with Matplotlib, and enhancing visual appeal.

## Excel

### Basic and Advanced Functionalities

-   **Basic Functionalities:**
    -   Creating tables and charts.
    -   Implementing conditional formatting.
    -   Using nested IF statements for complex logical operations.
-   **Advanced Functions:**
    -   **XLOOKUP:** Efficiently searching for and retrieving data across large datasets.
    -   **COUNTIF:** Counting cells that meet specific criteria.
    -   **IFERROR:** Handling errors gracefully in formulas.
    -   **SUMPRODUCT:** Performing array calculations for more complex data analysis.
    -   **Data Validation:** Ensuring data integrity and consistency within spreadsheets.

### Pivot Tables and Pivot Charts

I am highly proficient in using Pivot Tables and Pivot Charts, which are essential for summarizing, analyzing, and visualizing data. These tools have been integral in my professional work for performing descriptive analytics, such as analyzing inventory levels, past statuses, and conducting ABC analyses to identify trends in usage.

### Power Query for ETL

I frequently utilize Power Query for Extract, Transform, Load (ETL) functions, which allows me to automate data processing workflows. Power Query's capabilities enable me to:

-   Extract data from various sources.
-   Transform and clean the data.
-   Load the processed data into Excel for analysis.

This automation significantly enhances efficiency and accuracy in data handling.

### Professional Applications

In a professional setting, I have applied my Excel skills primarily for:

-   Descriptive analytics related to inventory management.
-   Conducting ABC analyses to classify inventory based on importance.
-   Identifying trends and patterns in data usage.

### Academic Projects

In school, I designed an ERP system within Excel for a small business simulation. This project involved:

-   Creating Bills of Materials (BOM) for two products.
-   Forecasting sales using dummy data.
-   Calculating safety stock levels.
-   Setting product prices based on estimated labor and material costs.

#### A note on VBA and Macros

While I have dabbled in VBA for creating macros, I prefer using standard Excel functions for most tasks to ensure that my tools are user-friendly and easily accessible to others. My experience with working and sharing files in Excel within Teams (formerly known as SharePoint) has further reduced the need for macros, as the collaborative environment supports most of my ETL requirements through Power Query.
:::

# Experience {#sec-experience .title}

## Data Scientist

[**RXA \@ OneMagnify**](https://rxa.io/) **- Remote, USA** \| January 2023 - Current

- **Amazon Competitive Marketplace Dashboard:** Led comprehensive data science projects with R & the {Tidyverse} framework, including a Tableau Dashboard for monitoring Parts Distributors' performance on Amazon, integrating sales, inventory, return, and review data, while also providing competitive marketplace analysis for Amazon with web scrapes and tools like Oxylabs, Helium10, and {httr2}.

- **Weekly Amazon Price Recommendation Report:** Implemented monthly Amazon marketplace scrapes to track knock-off parts and estimate sales and developed weekly price recommendation reports using R and Quarto, incorporating competitive data and pricing strategies. Report designed & rendered with {gt} and {ggplot2}.

- **Client Communication:** Maintained and strengthened client relationships, building rapport and technical trust through continuous communication, biweekly meetings, and regular email updates.

- **Internal Communication:** Set up an internal chat for R programming discussions, fostering active communication and debugging collaboration across teams. Led workshops on open-source tools like Quarto, implemented version control with Git/GitHub, standardized folder structures, and instituted reproducible environments with {renv}.

## Continuous Improvement Specialist (OLDP Rotation 2)

[**Xylem, Inc**](https://www.xylem.com/en-us/) **- Chicago, IL** \| July 2022 - December 2022

-   **Excess & Obsolete Inventory Identification & Disposition:** Employed the DMAIC/DMADV methodology to create a process aimed at reducing the site’s E&O financial reserve and material in warehouses by identifying obsolete items and creating opportunities for rework, resale, and disposal. The control plan included establishing a cross-departmental E&O taskforce, with the new process projected to achieve estimated hard savings of \$1M - \$4M over the next 3-5 years. Utilized skills in Process Flowcharts, Value Stream Mapping, and Control Charts.

-   **Ad-Hoc Floor Workshops:** Led and assisted various projects to drive Continuous Improvement initiatives at the site. These included 5S audits, lean methodology seminars, inventory audits, and cycle counts, applying skills in 5S, FMEA, and Process Capability Analysis.

## Business Data Analyst (OLDP Rotation 1)

[**Xylem, Inc**](https://www.xylem.com/en-us/) **- Uniontown, PA** \| July 2021 - July 2022

-   **Excess & Obsolete Reserve:** Designed a recommendation engine for the quarterly report, focusing on reducing ETL time and standardizing the process across all Sensus NA sites to comply with new financial policies. This effort reduced process time by 72 hours per month and enabled new reporting and forecasting capabilities. Leveraged Advanced Excel (Power Query, PivotTables, XLOOKUP) and Microsoft SQL Server.

-   **Sensus North America Physical Inventory:** Created documentation for the annual NA physical inventory event, including process flowcharts, work instructions, and manager checklists for tasks leading up to the event. This documentation package standardized the process across all NA sites and eliminated non-value-added activities during the event, utilizing skills in Project Management and Process Flowcharts.

-   **Ad-Hoc Reporting:** Developed various data pipelines, dashboards, and reports to meet unique business needs. This included tracking forecasting accuracy, comparing supplier and internal bill of materials, gathering and visualizing quality control measures, and synchronizing lot sizes across multiple sites.

## Undergraduate Teaching Assistant

[**West Virginia University**](https://www.wvu.edu/) **- Morgantown, WV** \| August 2019 - May 2021

As a Teaching Assistant at West Virginia University from August 2019 to May 2021, I contributed to the academic development of students across multiple courses in the Industrial Engineering department. I assisted in teaching:

-   **Spring 2021** \| IENG 220: Re-engineering Management Systems,
-   **Fall 2020** \| IENG 305: Intro to Systems Engineering,
-   **Spring 2020** \| IENG 331: Computer Applications in Industrial Engineering, and
-   **Fall 2019** \| IENG 445: Project Management for Engineers.

Throughout these roles, I gained valuable skills in teaching and curriculum development. My responsibilities included conducting lectures, leading lab sessions, grading assignments, and providing one-on-one support to students, helping them grasp complex concepts and apply theoretical knowledge to practical projects.

## Manufacturing Engineer Intern

[**JLG Industries**](https://www.jlg.com/) **- McConnellsburg, PA** \| June 2019 - August 2019

-   **Long-term Project, Test – Inspect – Green Tag (TIG) Line:** Conducted a comprehensive 6S audit, removing unnecessary equipment and parts from workstations and organizing the layout to enhance efficiency. I developed detailed Standard Work Instructions (SWIs) from scratch for all five stations of the TIG Line, covering three different products to ensure consistent and efficient processes.

-   **Short-term Project, Tire Manipulator:** Implemented a scanner-based system for defect entry into Electronic Quality Control (EQC), standardizing the process, reducing variability in defect types, and improving data collection. In a short-term project focused on the Tire Manipulator, I created detailed floor plans for current and future station layouts using laser measurements and AutoCAD.

# Education {#sec-education .title}

## Bachelor of Science in Industrial Engineering

**West Virginia University** \| Morgantown, WV

## Certifications

-   **Lean Six Sigma Green Belt** \| Institute of Industrial and Systems Engineers

-   **Continuous Improvement Fundamentals** \| Oshkosh Corporation

-   **Eligible for Certified Associate in Project Management (CAPM)** \| Project Management Institute
